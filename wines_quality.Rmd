---
title: "wine_quality"
author: "LJSSE"
date: "19/5/2021"
output: html_document
---

### Importing packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
library(broom)
library(ltm)
library(MASS)
```

################################################################################
## DATA

The dataset **white_wines.csv** stores data about physicochemical properties of many white wines coming from the north-west region, named Minho, of Portugal.

Each row refers to a wine, while each column contains a physicochemical property. 
Here we list the properties and their unit of measure:

- fixed acidity (FA: g(tartaric acid)/dm^3)
- volatile acidity (VA: g(acetic acid)/dm^3)
- citric acid (CA: g/dm^3)
- residual sugar (RS: g/dm^3)
- chlorides (CH: g(sodium chloride)/dm^3)
- free sulfur dioxide (FSD: mg/dm^3)
- total sulfur dioxide (TSD: mg/dm^3)
- density (DE: g/dm^3)
- pH
- sulphates (SU: g(potassium sulphate)/dm^3)
- alcohol (AL: %vol)
- quality (from 0 to 10)

Our aim is to classify white wines according to their quality, labeling them as "low" if their quality score is from 0 to 5, and as "high" if the score is between 6 and 10.


## Loading the dataset
We now inspect the variables, and transform them if necessary.

```{r, message=FALSE, warning = FALSE}
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
```

All the features are numeric, including the target.
We re-encode the target as a factor to make it binary, with two levels of quality.

## Defining the target

```{r, message=FALSE, warning = FALSE}
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
                                      wwines$quality > 5 ~ "high"))
table(wwines$quality)
```

```{r}
wwines["id"] <- rownames(wwines)
```

## Data description

Before proceeding with our analysis we perform some descriptive statistics, in order to both understand the relations within the variables, and to visualize their distributions.

### Heatmap

First we plot the correlation matrix, in order to detect any strong relation among features which could lead to multicollinearity problems.
```{r}
cors <- cor(wwines[-c(12,13)])
corrplot(cors, type = "upper", 
         tl.col = "black", tl.srt = 45)

```
This plot highlights a strong correlation between density and two variables: residual.sugar and alcohol.

### Histograms

To get an understanding of which variable should be excluded from the regression, we plot the histograms of the highly correlated variables identified above.
```{r}
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

density.plot <- ggplot(wwines, aes(density, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)


grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)

```
We decide to remove density. In fact, we can observe that for each value of density there is no difference in the distribution of wines' quality, once we account for the numerosity of the two quality levels. On the other hand, the modal values for the other two variables differ for low and high quality wines, thus adding more useful information for the purpose of classification.

```{r}
wwines$density <- NULL
```


### Correlation with target variable

It is also useful to compute the correlations between the covariates and the target, in order to see if there are any particularly powerful features that would make the task too trivial.
```{r, warning=FALSE, message=FALSE}
wwines$quality <- factor(wwines$quality, levels = c("high","low"))
bicorrs <- sapply(wwines[,-c(11,12)], function(x) round(biserial.cor(x,wwines$quality),2))
data.frame(bicorrs) %>% arrange(abs(bicorrs))
```
Here the correlations are displayed in ascending absolute value, and we can observe that the variable showing the highest correlation with wines' quality is alcohol, but this is not a too strong correlation.
The sign is positive, meaning that as alcohol increases the quality of wine also increases.

### Boxplots

We now look at the distribution of all the explanatory variables, in order both to visualize those values which are far from the bulk of the data, and to get an understanding of the variables displaying the most divergent median values for low and high quality wines.
```{r, message=FALSE, warning = FALSE}
summary(wwines)
# melt the dataset
wwines.m <- melt(wwines[,-12], id.var = "quality")

# wrapped boxplots
ggplot(data = wwines.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=quality)) +
facet_wrap( ~ variable, scales="free")
```

We can observe the following:
- The features displaying the highest median difference between high and low quality wines are volatile.acidity, total.sulfur.dioxide, pH, alcohol. The variables are candidates to be the significant ones in the regression framework.
- There are some outlying values. We will deal with them in the section below.

## Data splitting

```{r}
set.seed(42)
train_indices<- createDataPartition(wwines$quality,p=0.7,list=FALSE)
dtrain <- wwines[train_indices,]
dtest <- wwines[-train_indices,]
```

###########################################################################

# VARIABLES SELECTION

## Forward Stepwise method
```{r}
library(leaps)
regfit.fwd=regsubsets(quality~.,data=wwines[,-12],method="forward", nvmax=13)
summary.fwd <- summary(regfit.fwd)

plot(summary.fwd$bic,xlab="Number of Variables",ylab="bic")
points(which.min(summary.fwd$bic),summary.fwd$bic[which.min(summary.fwd$bic)],pch=20,col="red")
```
Forward stepwise steps:
1. inclusion of alcohol
2. inclusion of volatile.acidity
3. inclusion of residual.sugar
4. inclusion of sulphates
5. inclusion of fixed.acidity
6. inclusion of free.sulfur.dioxide
7. inclusion of total.sulfur.dioxide
8. inclusion of pH
9. inclusion of chlorides 
10. inclusion of citric.acid

Optimal number of variables: 6.

```{r}
plot(regfit.fwd,scale="bic")
grid(nx=11,ny=10,col="red",lty = "solid")
abline(h=10, col="yellow", lwd = 10)
text(10.5,9, "Optimal model", col="yellow",cex = 0.8)
```

# LOGISTIC REGRESSION

The logistic regression is now performed on the restricted model with the six selected variables:
fixed.acidity, volatile.acidity, residual.sugar, free.sulfur.dioxide, sulphates, alcohol.

With factor() we relevel the variable "quality" in order to set "low" as the baseline for the logistic regression.
```{r}
wwines$quality <- factor(wwines$quality, levels = c("low","high"))

dtrain.res <- dtrain[,c(1,2,4,6,9,10,11,12)]
dtest.res <- dtest[,c(1,2,4,6,9,10,11,12)]

lr_fit_res <- glm(quality ~., data =  dtrain.res[,-8],
          family=binomial(link='logit'))

# coefficients
summary(lr_fit_res)

# odds ratios
exp(cbind(OR = coef(lr_fit_res), confint(lr_fit_res)))
```
The following variables are those significant at a 99% confidence level:
- Intercept: the value 440232.6% corresponds to the increase in the odds of having a high quality wine when all the explanatory variables are set to 0.
- fixed.acidity: a unitary increase (1 g/dm^3) in volatile acidity will lead to a 23.13% increase in the odds of having a high quality wine.
- volatile.acidity: a unitary increase (1 g/dm^3) in volatile acidity will lead to a 102318.1% increase in the odds of having a high quality wine.
- residual.sugar: a unitary increase (1 g/dm^3) in volatile acidity will lead to a 6.76% decrease in the odds of having a high quality wine.
- free.sulfur.dioxide: a unitary increase (1 mg/dm^3) in volatile acidity will lead to a 0.62% decrease in the odds of having a high quality wine.
- alcohol: a unitary increase (percentage) in volatile acidity will lead to a 66.04% decrease in the odds of having a high quality wine.

The following variable is significant at a 95% confidence level:
- sulphates: a unitary increase (1 g/dm^3) in volatile acidity will lead to a 73.22% decrease in the odds of having a high quality wine.


## Optimal threshold

In order to get the best possible performance, we performed threshold optimization exploiting the mlr3 package. 
Since the choice of the threshold strictly depends on the aim of the analysis, which in our case is to build a predictor which would have the best behaviour on new different data, we looked for a threshold which could guarantee an equilibrium between the two types of errors.

```{r}
#here we use mlr3 package
#defining our task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
#defining the type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
#defining the resampling method
cv5 = rsmp("cv", folds = 5)
#defing the search space
thresholds <- seq(0.5,0.8,0.01)

#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)

for (thresh in thresholds) {
    res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
    #combined prediction of all individual resampling iterations
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    #computing the scores
    scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
    coefficients <- (unname(scores))
    measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}

measures <- data.frame(thresholds,
                       "accuracy" = unlist(measures_list[[1]]), 
                       "sensitivity" = unlist(measures_list[[2]]),
                       "specificity" = unlist(measures_list[[3]]))

#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th <- mean(thresholds[intersection_indices]) 

melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = th,linetype = "dotted") +
  geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))

```
The optimal threshold for our aim is 0.65.
This thresholds meets our expectations, as the equilibrium point corresponds to a value higher than 0.5. 
This higher threshold compensates for the higher a priori probability of labelling a wine as high quality (low: 1640, high: 3258).

## Model's test performance 


```{r}
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
task_test = TaskClassif$new(id = "wines_test",  dtest.res[,-8], target = "quality", positive = "high")

learner = lrn("classif.log_reg", predict_type = "prob")
cv5 = rsmp("cv", folds = 5)

learner$train(task_train) # training on train set
base_pred <- learner$predict(task_test) # predicting on test set
base_pred$set_threshold(th)

# performance
cm_base <- list("confusion" = base_pred$confusion,
          "accuracy" = base_pred$score(measures = msr("classif.acc")),
          "sensitivity"=base_pred$score(measures = msr("classif.sensitivity")),
          "specificity"=base_pred$score(measures = msr("classif.specificity")))
cm_base
```

### Full/Restricted Performance comparison 

In order to see how the model's performance is affected by the dimensionality reduction, we compute the predictions for both the restricted and the full model and we compare the values of accuracy, sensitivity, and specificity.
By removing 5 out of 11 explanatory variables we are trying to avoid overfitting, thus creating a more stable model with a lower variance; of course, we expect the performance to be slighlty worse, as we are introducing some bias.


```{r, warning=FALSE}
#running the full model
lr_fit <- glm(quality ~., data =  dtrain[,-12],
          family=binomial(link='logit'))

#full model performance
unrestr_probs <- predict(lr_fit,  dtest[,-12], type="response")
unrestr_preds <- ifelse(unrestr_probs > th,"high","low")
unrestr_cm <- confusionMatrix(factor(unrestr_preds),
                         dtest$quality,
                        positive = "high"
                        )
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])

#restricted model performance
restr_probs <- predict(lr_fit_res,  dtest.res[,-8], type="response")
restr_preds <- ifelse(restr_probs > th,"high","low")
restr_cm <- confusionMatrix(factor(restr_preds),                         dtest$quality,
                        positive = "high"
                        )
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])


data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance), 
           "difference" = unlist(restr_performance)-unlist(unrestr_performance))
```
As expected there is a slight worsening in the model's prediction accuracy, but this increase in bias is negligible with respect to the decrease in variance obtained with the simpler model.


## Model diagnostics

### Multicollinearity 
Variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables.


```{r}
a <- vif(lr_fit_res)
sqrt(vif(lr_fit_res))>2
```

Checking the VIF after the variable "density" removal we can confirm that no other variable is influenced by the others.

### Detecting outliers

```{r warning=FALSE}
#see Cook's distance
plot(lr_fit_res, which = 4, id.n = 1)
```

We remove the observation with a very large cook's distance (id = 4746), and inspect the plot again, to see the variablitiy of the remaining computed distances.

```{r}
dtrain_of <- dtrain.res %>% filter(id != "4746")

lr_fit_of_1 <- glm(quality ~., data =  dtrain_of[,-8],
          family=binomial(link='logit'))

plot(lr_fit_of_1, which = 4, id.n = 0)
```
From this second plot we can see that there are some observations with a larger cook's distance with respect to the others, but they are within an acceptable range of variability, and we thus keep them.


## Outliers free model

Now we build a new model without the outlying observation.
```{r}
#fitting on outlier free data
lr_fit_outlierfree <- glm(quality ~., data =  dtrain_of[,-8],family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```

### Performance

As always, it is useful to know if the processing we do improves our model: here we start producing performance measures.
```{r}
task_train = TaskClassif$new(id = "wines_outlierfree",  dtrain_of[,-8], target = "quality", positive = "high")

learner = lrn("classif.log_reg", predict_type = "prob")

cv5 = rsmp("cv", folds = 5)

task_test = TaskClassif$new(id = "wines_outlierfree_test",  dtest.res[,-8], target = "quality", positive = "high")

learner$train(task_train)

outlierfree_pred <- learner$predict(task_test)
# set optimal threshold
outlierfree_pred$set_threshold(th)
cm_outlierfree <- list("confusion" = outlierfree_pred$confusion,
          "accuracy" = outlierfree_pred$score(measures = msr("classif.acc")),
          "sensitivity"=outlierfree_pred$score(measures = msr("classif.sensitivity")),
          "specificity"=outlierfree_pred$score(measures = msr("classif.specificity")))

cm_outlierfree
```

### Performance comparison

In the following table we show the comparison between the newest model we produced and the base model from which we started.
```{r}
data.frame("outlierfree_performance" = unlist(cm_outlierfree[-1]),"base_performance" = unlist(cm_base[-1]), 
           "improvement" = unlist(cm_outlierfree[-1])-unlist(cm_base[-1]))
```
It seems like removing outliers could improve a bit the performance, thus we keep using the outlier-free model from now on.

### Estimated coefficients

Let's see how much the coefficients change from model to model.
```{r}
base <- data.frame(summary(lr_fit_res)$coefficient)
no_outliers<- data.frame(summary(lr_fit_outlierfree)$coefficient)

variable<-c("(constant)",colnames(dtrain.res)[-c(7,8)])

data.frame(variable,base$Estimate,no_outliers$Estimate, "difference"=base$Estimate-no_outliers$Estimate)
```
We can notice that the coefficients don't change too much when removing outliers. The only one which shifts a little bit more is the coefficient relative to the intercept, which has a -0.11 change. It could be that the outlier we removed affected the intercept coefficient estimate the most.

# OPTIMAL THRESHOLD

After removing outliers we are worried about the goodness of the threshold we have found previously, thus we test it again.
```{r}
#we want to tweak the threshold of our classification: we analyze values from 0.3 to 0.6 with step 0.01
thresholds <- seq(0.5,0.8,0.01)

#collecting performances with different thresholds
measures_list <- rep(list(list()), 3)

for (thresh in thresholds) {
    res_cv = resample(task_train, learner, cv5, store_models = TRUE)
    #combined prediction of all individual resampling iterations
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    #scores are combined as well from all individual resampling iterations
    scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
    coefficients <- (unname(scores))
    measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}

measures <- data.frame(thresholds,
                       "accuracy" = unlist(measures_list[[1]]), 
                       "sensitivity" = unlist(measures_list[[2]]),
                       "specificity" = unlist(measures_list[[3]]))

#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th <- mean(thresholds[intersection_indices]) 

melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = th,linetype = "dotted") +
  geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))

```

The threshold is not affected by the outlier removal, thus we keep using the same threshold as before.

################################################################################
# BAGGING 
```{r}
n <- seq(nrow( dtrain))
set.seed(1)
S1 <- sample(n, nrow( dtrain), replace = TRUE)
set.seed(2)
S2 <- sample(n, nrow( dtrain), replace = TRUE)
set.seed(3)
S3 <- sample(n, nrow( dtrain), replace = TRUE)
set.seed(4)
S4 <- sample(n, nrow( dtrain), replace = TRUE)
lr_b <- function(i){
      lr_f <- glm(quality ~ fixed.acidity+volatile.acidity+residual.sugar+free.sulfur.dioxide+sulphates+alcohol, 
                  data =  dtrain[i, ],family=binomial(link='logit'))
      lr_prob <- predict(lr_f, dtrain[i,], type="response")
      lr_pred <- ifelse(lr_prob > th,"high","low")
      cm <- confusionMatrix(
                as.factor(lr_pred),
                 dtrain[i,]$quality,
                positive = "high"
                )
      a <- cm$overall[[1]]
      return(a)
}
aggregate = (lr_b(S1) +lr_b(S2) + lr_b(S3) + lr_b(S4))/4
round(aggregate,4)
```
{output will be ROC curve}

################################################################################

